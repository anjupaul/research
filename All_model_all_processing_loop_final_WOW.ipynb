{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1drmd8k8kMQMO3vW40ZYHzz4LhBV7Afui"},"id":"rQTnM9DMCOBi","outputId":"629f984f-5a1e-4792-ab6b-af2e0fa9f8d7"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import yfinance as yf\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","from sklearn.model_selection import GridSearchCV\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM, Dropout, LayerNormalization, Flatten, Conv1D\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.ensemble import RandomForestRegressor\n","import tensorflow_probability as tfp\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, MultiHeadAttention\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Function to handle outliers using IQR method\n","def remove_outliers(df, column):\n","    Q1 = df[column].quantile(0.25)\n","    Q3 = df[column].quantile(0.75)\n","    IQR = Q3 - Q1\n","    lower_bound = Q1 - 1.5 * IQR\n","    upper_bound = Q3 + 1.5 * IQR\n","    return df[(df[column] \u003e= lower_bound) \u0026 (df[column] \u003c= upper_bound)]\n","\n","# Load the dataset\n","ticker = 'WOW.AX'\n","data = yf.download(ticker, start='2000-01-01', end='2024-01-01')\n","data = data[['Close']].iloc[-2000:]  # Use the last 2000 rows\n","data = data.dropna()\n","\n","# Exploratory Data Analysis (EDA)\n","print(\"Performing EDA...\")\n","plt.figure(figsize=(10, 6))\n","plt.title(\"Stock Closing Prices\")\n","plt.plot(data['Close'])\n","plt.show()\n","\n","# Handling outliers\n","data = remove_outliers(data, 'Close')\n","\n","# Univariate analysis\n","plt.figure(figsize=(10, 6))\n","sns.histplot(data['Close'], kde=True)\n","plt.title(\"Distribution of Closing Prices\")\n","plt.show()\n","\n","# Bivariate analysis\n","plt.figure(figsize=(10, 6))\n","sns.boxplot(data['Close'])\n","plt.title(\"Boxplot of Closing Prices\")\n","plt.show()\n","\n","# Split the data\n","def split_data(data, sequence_length=60):\n","    X, y = [], []\n","    for i in range(sequence_length, len(data)):\n","        X.append(data.iloc[i-sequence_length:i].values)\n","        y.append(data.iloc[i].values[0])\n","    X, y = np.array(X), np.array(y)\n","    split = int(0.8 * len(X))\n","    X_train, X_test = X[:split], X[split:]\n","    y_train, y_test = y[:split], y[split:]\n","    split_val = int(0.8 * len(X_train))\n","    X_train, X_val = X_train[:split_val], X_train[split_val:]\n","    y_train, y_val = y_train[:split_val], y_train[split_val:]\n","    return X_train, y_train, X_val, y_val, X_test, y_test\n","\n","# Preprocessing functions\n","def preprocess_standard_scaler(X_train, X_val, X_test):\n","    scaler = StandardScaler()\n","    X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n","    X_val = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n","    X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n","    return X_train, X_val, X_test\n","\n","def preprocess_minmax_scaler(X_train, X_val, X_test):\n","    scaler = MinMaxScaler()\n","    X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n","    X_val = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n","    X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n","    return X_train, X_val, X_test\n","\n","# Model Definitions with higher dropout and alternative activation functions\n","def build_lstm_model(input_shape):\n","    model = Sequential([\n","        tf.keras.layers.Input(shape=input_shape),\n","        LSTM(30, return_sequences=True, activation='relu'),\n","        Dropout(0.2),  # Increased dropout\n","        LSTM(30, return_sequences=False, activation='relu'),\n","        Dropout(0.2),  # Increased dropout\n","        Dense(1)\n","    ])\n","    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n","    return model\n","\n","# Define Transformer model\n","def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n","    # Normalization and Attention\n","    x = LayerNormalization(epsilon=1e-6)(inputs)\n","    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n","    x = Dropout(dropout)(x)\n","    res = x + inputs\n","\n","    # Feed Forward Part\n","    x = LayerNormalization(epsilon=1e-6)(res)\n","    x = Dense(ff_dim, activation=\"relu\")(x)\n","    x = Dropout(dropout)(x)\n","    x = Dense(inputs.shape[-1])(x)\n","    return x + res\n","\n","def build_model_transformer(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n","    inputs = Input(shape=input_shape)\n","    x = inputs\n","    for _ in range(num_transformer_blocks):\n","        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n","\n","    x = Flatten()(x)\n","    for dim in mlp_units:\n","        x = Dense(dim, activation=\"relu\")(x)\n","        x = Dropout(mlp_dropout)(x)\n","    outputs = Dense(1)(x)\n","    return Model(inputs, outputs)\n","\n","def build_tcn_model(input_shape):\n","    model = Sequential([\n","        Input(shape=input_shape),\n","        Conv1D(filters=32, kernel_size=3, padding=\"causal\", activation=\"relu\"),\n","        Conv1D(filters=32, kernel_size=3, padding=\"causal\", activation=\"relu\"),\n","        Flatten(),\n","        Dense(128, activation='relu'),\n","        Dropout(0.4),  # Increased dropout\n","        Dense(1)\n","    ])\n","    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n","    return model\n","\n","def build_rf_model(seed_value):\n","    model = RandomForestRegressor(n_estimators=100, random_state=seed_value)\n","    return model\n","\n","# Hyperparameter tuning for Random Forest using Grid Search\n","def tune_rf_hyperparameters(X_train, y_train, seed_value):\n","    rf = RandomForestRegressor(random_state=seed_value)\n","    param_grid = {\n","        'n_estimators': [50, 100, 200],\n","        'max_features': ['auto', 'sqrt', 'log2'],\n","        'max_depth': [10, 20, None]\n","    }\n","    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n","    grid_search.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n","    print(\"Best parameters found: \", grid_search.best_params_)\n","    return grid_search.best_estimator_\n","\n","# Early stopping to avoid overfitting\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","# Training and Evaluation Function\n","def train_and_evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, model_type, seed_value):\n","    if model_type.startswith('RF'):\n","        model.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n","        predictions = model.predict(X_test.reshape(X_test.shape[0], -1))\n","    else:\n","        history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping])\n","        predictions = model.predict(X_test)\n","        plt.figure(figsize=(10, 5))\n","        plt.plot(history.history['loss'], label='Training Loss')\n","        plt.plot(history.history['val_loss'], label='Validation Loss')\n","        plt.xlabel('Epochs')\n","        plt.ylabel('Loss')\n","        plt.title(f'Training and Validation Loss - {model_type}')\n","        plt.legend()\n","        plt.show()\n","\n","    mse = mean_squared_error(y_test, predictions)\n","    mae = mean_absolute_error(y_test, predictions)\n","    r2 = r2_score(y_test, predictions)\n","    print(f'RMSE {model_type}: {np.sqrt(mse)}')\n","    print(f'MAE {model_type}: {mae}')\n","    print(f'RÂ² {model_type}: {r2}')\n","\n","    plt.figure(figsize=(14, 7))\n","    plt.plot(y_test, label='Actual Prices')\n","    plt.plot(predictions, label='Predicted Prices')\n","    plt.title(f'Actual vs Predicted Stock Prices - {model_type}')\n","    plt.xlabel('Time')\n","    plt.ylabel('Stock Price')\n","    plt.legend()\n","    plt.show()\n","\n","    return {\n","        'Seed': seed_value,\n","        'Model': model_type,\n","        'RMSE': np.sqrt(mse),\n","        'MAE': mae,\n","        'R2': r2\n","    }\n","\n","# Loop for multiple seed values and record the results\n","results = []\n","for run in range(10):\n","    seed_value = 42 + run\n","    np.random.seed(seed_value)\n","    tf.random.set_seed(seed_value)\n","\n","    X_train, y_train, X_val, y_val, X_test, y_test = split_data(data)\n","\n","    # No Preprocessing\n","    print(f\"No Preprocessing - Seed {seed_value}\")\n","    lstm_model = build_lstm_model((X_train.shape[1], X_train.shape[2]))\n","    results.append(train_and_evaluate_model(lstm_model, X_train, y_train, X_val, y_val, X_test, y_test, 'LSTM_NoPreprocessing', seed_value))\n","\n","    transformer_model = build_model_transformer((X_train.shape[1], X_train.shape[2]), head_size=256, num_heads=4, ff_dim=4, num_transformer_blocks=4, mlp_units=[128], dropout=0.1, mlp_dropout=0.1)\n","    transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='mean_squared_error')\n","    results.append(train_and_evaluate_model(transformer_model, X_train, y_train, X_val, y_val, X_test, y_test, 'Transformer_NoPreprocessing', seed_value))\n","\n","    tcn_model = build_tcn_model((X_train.shape[1], X_train.shape[2]))\n","    results.append(train_and_evaluate_model(tcn_model, X_train, y_train, X_val, y_val, X_test, y_test, 'TCN_NoPreprocessing', seed_value))\n","\n","    rf_model = tune_rf_hyperparameters(X_train, y_train, seed_value)\n","    results.append(train_and_evaluate_model(rf_model, X_train, y_train, X_val, y_val, X_test, y_test, 'RF_NoPreprocessing', seed_value))\n","\n","    # Standard Scaling\n","    print(f\"Standard Scaling - Seed {seed_value}\")\n","    X_train_scaled, X_val_scaled, X_test_scaled = preprocess_standard_scaler(X_train, X_val, X_test)\n","\n","    lstm_model = build_lstm_model((X_train_scaled.shape[1], X_train_scaled.shape[2]))\n","    results.append(train_and_evaluate_model(lstm_model, X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test, 'LSTM_Standard', seed_value))\n","\n","    transformer_model = build_model_transformer((X_train_scaled.shape[1], X_train_scaled.shape[2]), head_size=256, num_heads=4, ff_dim=4, num_transformer_blocks=4, mlp_units=[128], dropout=0.1, mlp_dropout=0.1)\n","    transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='mean_squared_error')\n","    results.append(train_and_evaluate_model(transformer_model, X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test, 'Transformer_Standard', seed_value))\n","\n","    tcn_model = build_tcn_model((X_train_scaled.shape[1], X_train_scaled.shape[2]))\n","    results.append(train_and_evaluate_model(tcn_model, X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test, 'TCN_Standard', seed_value))\n","\n","    rf_model = tune_rf_hyperparameters(X_train_scaled, y_train, seed_value)\n","    results.append(train_and_evaluate_model(rf_model, X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test, 'RF_Standard', seed_value))\n","\n","    # Min-Max Scaling\n","    print(f\"Min-Max Scaling - Seed {seed_value}\")\n","    X_train_minmax, X_val_minmax, X_test_minmax = preprocess_minmax_scaler(X_train, X_val, X_test)\n","\n","    lstm_model = build_lstm_model((X_train_minmax.shape[1], X_train_minmax.shape[2]))\n","    results.append(train_and_evaluate_model(lstm_model, X_train_minmax, y_train, X_val_minmax, y_val, X_test_minmax, y_test, 'LSTM_MinMax', seed_value))\n","\n","    transformer_model = build_model_transformer((X_train_minmax.shape[1], X_train_minmax.shape[2]), head_size=256, num_heads=4, ff_dim=4, num_transformer_blocks=4, mlp_units=[128], dropout=0.1, mlp_dropout=0.1)\n","    transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='mean_squared_error')\n","    results.append(train_and_evaluate_model(transformer_model, X_train_minmax, y_train, X_val_minmax, y_val, X_test_minmax, y_test, 'Transformer_MinMax', seed_value))\n","\n","    tcn_model = build_tcn_model((X_train_minmax.shape[1], X_train_minmax.shape[2]))\n","    results.append(train_and_evaluate_model(tcn_model, X_train_minmax, y_train, X_val_minmax, y_val, X_test_minmax, y_test, 'TCN_MinMax', seed_value))\n","\n","    rf_model = tune_rf_hyperparameters(X_train_minmax, y_train, seed_value)\n","    results.append(train_and_evaluate_model(rf_model, X_train_minmax, y_train, X_val_minmax, y_val, X_test_minmax, y_test, 'RF_MinMax', seed_value))\n","\n","# Save the results to a CSV file\n","results_df = pd.DataFrame(results)\n","results_df.to_csv('model_results.csv', index=False)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOVyUFjzExlfva/M3Lri8Aa","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}